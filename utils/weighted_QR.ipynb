{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape = torch.Size([6, 3])\n",
      "Q.shape = torch.Size([6, 3])\n",
      "R.shape = torch.Size([3, 3])\n",
      "Reconstruction error (rows with W>0) = 1.3677447441295953e-07\n",
      "||Q^T W Q - I|| on subspace (nonzero rows) = 1.88081486385272e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def weighted_qr_ignore_zero_rows(A, W):\n",
    "    \"\"\"\n",
    "    Perform a 'weighted QR' on matrix A w.r.t. diag(W), ignoring rows where W=0.\n",
    "    Returns Q, R such that:\n",
    "      1) A = Q R   (reconstructs only the rows where W>0 exactly)\n",
    "      2) Q^T diag(W) Q = I in the subspace of nonzero W.\n",
    "         Rows with W=0 get filled with zeros in Q by default.\n",
    "    \"\"\"\n",
    "    # A: (m, n)\n",
    "    # W: (m,) diagonal entries, may have zeros\n",
    "    assert A.shape[0] == W.shape[0], \"Mismatched shapes between A and W.\"\n",
    "\n",
    "    # 1) Identify nonzero-weight rows\n",
    "    mask = (W > 0)\n",
    "    \n",
    "    # 2) Extract submatrix and subweights\n",
    "    A_sub = A[mask, :]             # shape (m_sub, n)\n",
    "    W_sub = W[mask]                # shape (m_sub,)\n",
    "\n",
    "    # If no zero weights, just do a normal weighted QR\n",
    "    if A_sub.shape[0] == A.shape[0]:\n",
    "        # i.e., mask is all True\n",
    "        # standard weighted qr (assumes W_sub all positive)\n",
    "        return weighted_qr(A, W)   # from the previous helper you wrote\n",
    "\n",
    "    # 3) Compute Weighted QR on submatrix\n",
    "    Q_sub, R = weighted_qr(A_sub, W_sub)  # as previously defined\n",
    "\n",
    "    # 4) Reassemble full-size Q\n",
    "    m, n = A.shape\n",
    "    Q = A.new_zeros((m, n))  # same dtype/device\n",
    "    Q[mask, :] = Q_sub       # put back the valid rows\n",
    "\n",
    "    return Q, R\n",
    "\n",
    "def weighted_qr(A, W):\n",
    "    \"\"\"\n",
    "    Standard Weighted QR: A = Q R, with Q^T diag(W) Q = I.\n",
    "    (Requires that W > 0 on all diagonal entries.)\n",
    "    \"\"\"\n",
    "    W_sqrt = W.sqrt()      # shape (m,)\n",
    "    W_inv_sqrt = 1.0 / W_sqrt\n",
    "\n",
    "    # Multiply each row i by sqrt(W[i])\n",
    "    B = W_sqrt.unsqueeze(-1) * A\n",
    "    # Standard QR on B\n",
    "    Q_std, R = torch.linalg.qr(B, mode='reduced')\n",
    "    # Map Q_std back\n",
    "    Q = W_inv_sqrt.unsqueeze(-1) * Q_std\n",
    "    return Q, R\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage\n",
    "m, n = 6, 3\n",
    "A = torch.randn(m, n)\n",
    "# Suppose some rows have zero weight\n",
    "W_vals = torch.tensor([1.0, 0.0, 2.5, 0.0, 0.1, 0.0])  \n",
    "\n",
    "Q, R = weighted_qr_ignore_zero_rows(A, W_vals)\n",
    "\n",
    "print(\"A.shape =\", A.shape)\n",
    "print(\"Q.shape =\", Q.shape)\n",
    "print(\"R.shape =\", R.shape)\n",
    "A_approx = Q @ R\n",
    "\n",
    "# Reconstruction check on the nonzero-weight rows\n",
    "mask = (W_vals > 0)\n",
    "err_reconstruction = (A_approx[mask] - A[mask]).norm().item()\n",
    "print(\"Reconstruction error (rows with W>0) =\", err_reconstruction)\n",
    "\n",
    "# Weighted orthogonality check on subspace\n",
    "W_mat = torch.diag(W_vals)\n",
    "check = Q.T @ W_mat @ Q  # shape (n, n)\n",
    "I_n = torch.eye(n, dtype=Q.dtype, device=Q.device)\n",
    "err_QTWQ = (check - I_n).norm().item()\n",
    "print(\"||Q^T W Q - I|| on subspace (nonzero rows) =\", err_QTWQ)\n",
    "\n",
    "# For rows where W=0, we didn't constrain Q at all, so Q is zero in those rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((0,1,2,3))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/g5qd8cf95dg743cjhmdb6zsw0000gn/T/ipykernel_33788/2318002198.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W = torch.diag(torch.tensor(weights[mask]))\n",
      "/var/folders/9z/g5qd8cf95dg743cjhmdb6zsw0000gn/T/ipykernel_33788/2318002198.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W_inv = torch.diag(1/torch.tensor(weights[mask]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [5] at index 0 does not match the shape of the indexed tensor [4, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m alpha \u001b[38;5;241m=\u001b[39m R_full \u001b[38;5;241m@\u001b[39m alpha\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(alpha\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m/\u001b[39m (\u001b[43malpha\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m alpha[mask])\n\u001b[1;32m     37\u001b[0m alpha \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(true_energy))\n\u001b[1;32m     39\u001b[0m Q_full \u001b[38;5;241m=\u001b[39m W_inv_sqrt \u001b[38;5;241m@\u001b[39m Q_full\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [5] at index 0 does not match the shape of the indexed tensor [4, 1] at index 0"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([1,0,3,4,5])\n",
    "\n",
    "mask = weights > 0\n",
    "\n",
    "W_og = torch.diag(weights).float()\n",
    "\n",
    "W = torch.diag(torch.tensor(weights[mask]))\n",
    "W_inv = torch.diag(1/torch.tensor(weights[mask]))\n",
    "\n",
    "W_sqrt = W.sqrt()      # shape (batch_size, m)\n",
    "W_inv_sqrt = W_inv.sqrt()      # shape (batch_size, m)\n",
    "\n",
    "\n",
    "alpha = torch.ones(2,1) * 2\n",
    "\n",
    "true_energy = 3\n",
    "\n",
    "A = torch.randn(5, 2)\n",
    "\n",
    "A_mask = A[mask, :]\n",
    "\n",
    "A = W_sqrt @ A_mask\n",
    "\n",
    "Q, R = torch.linalg.qr(A, mode='reduced')\n",
    "\n",
    "Q_full = torch.zeros((4,4))\n",
    "R_full = torch.zeros((4,2))\n",
    "\n",
    "\n",
    "Q_full[:,:2] = Q\n",
    "R_full[:2,:] = R\n",
    "\n",
    "\n",
    "alpha = R_full @ alpha\n",
    "print(alpha.shape)\n",
    "alpha = alpha / (alpha.T @ alpha)\n",
    "alpha = alpha * torch.sqrt(torch.tensor(true_energy))\n",
    "\n",
    "Q_full = W_inv_sqrt @ Q_full\n",
    "\n",
    "print((Q_full @ R_full).shape)\n",
    "print((Q_full @ alpha).shape)\n",
    "\n",
    "full_QRa = torch.zeros((5, 1)).float()\n",
    "full_QRa[mask, :] = Q_full @ alpha\n",
    "\n",
    "energy = (full_QRa).T @ W_og @ (full_QRa)\n",
    "\n",
    "print(Q_full @ R_full)\n",
    "print(A)\n",
    "print(energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error on nonzero-weight rows: 3.039e-07\n",
      "Q^T W Q =\n",
      "tensor([[1.0000e+00, 3.7253e-09],\n",
      "        [1.8626e-09, 1.0000e+00]])\n",
      "Final energy = 3.000 (target = 3.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Setup\n",
    "weights = torch.tensor([1, 0, 3, 4, 5], dtype=torch.float32)\n",
    "mask = (weights > 0)               # e.g. [True, False, True, True, True]\n",
    "\n",
    "W_og = torch.diag(weights)         # (5x5) original weighting\n",
    "true_energy = 3.0\n",
    "\n",
    "A_orig = torch.randn(5, 2)         # The original full A\n",
    "\n",
    "# 2) Submatrix for nonzero weights\n",
    "A_sub = A_orig[mask, :]            # shape (4,2)\n",
    "w_sub = weights[mask]              # shape (4,)\n",
    "\n",
    "# 3) Weighted matrix B = W_sqrt * A_sub\n",
    "W_sqrt = torch.diag(w_sub.sqrt())         # (4x4)\n",
    "B = W_sqrt @ A_sub                        # (4x2)\n",
    "\n",
    "# 4) Standard QR on B\n",
    "Q_std, R = torch.linalg.qr(B, mode='reduced')\n",
    "# Q_std is (4,2),  R is (2,2)\n",
    "\n",
    "# 5) Weighted Q_sub = W_inv_sqrt @ Q_std\n",
    "W_inv_sqrt = torch.diag(1.0 / w_sub.sqrt())\n",
    "Q_sub = W_inv_sqrt @ Q_std   # (4,2), and Q_sub^T diag(w_sub) Q_sub = I\n",
    "\n",
    "# 6) Insert Q_sub into a full Q_full that has 5 rows\n",
    "Q_full = torch.zeros((5, 2), dtype=A_orig.dtype)\n",
    "Q_full[mask, :] = Q_sub  # fill the 4 rows that matter\n",
    "\n",
    "# Check: Q_sub@R ~ A_sub\n",
    "recon_sub = Q_sub @ R\n",
    "recon_error_sub = (recon_sub - A_sub).norm().item()\n",
    "\n",
    "print(f\"Reconstruction error on nonzero-weight rows: {recon_error_sub:.3e}\")\n",
    "\n",
    "# 7) Example alpha\n",
    "alpha = torch.ones((2,1)) * 2  # shape (2,1), just a test\n",
    "# Suppose you want to normalize alpha so that (Q_full alpha) has energy = true_energy\n",
    "\n",
    "# Weighted norm = alpha^T (Q_full^T W_og Q_full) alpha\n",
    "# But Q_full^T W_og Q_full = \"I\" in the subspace, and 0 for the zero row\n",
    "# So effectively it's alpha^T alpha. Let's check:\n",
    "\n",
    "M = Q_full.T @ W_og @ Q_full  # shape (2,2)\n",
    "# If only 1 zero weight row, rank might still be 2. Let's see:\n",
    "print(\"Q^T W Q =\")\n",
    "print(M)\n",
    "\n",
    "# 8) Normalize alpha\n",
    "current_energy = alpha.T @ M @ alpha\n",
    "alpha = alpha / torch.sqrt(current_energy)  # => energy is now 1\n",
    "alpha = alpha * torch.sqrt(torch.tensor(true_energy))\n",
    "# => energy is now true_energy\n",
    "\n",
    "# 9) form the final vector = Q_full @ alpha\n",
    "full_QRa = Q_full @ alpha    # shape (5,1)\n",
    "# Weighted energy:\n",
    "energy = (full_QRa.T @ W_og @ full_QRa).item()\n",
    "\n",
    "print(f\"Final energy = {energy:.3f} (target = {true_energy})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " tensor([[ 1.2382, -0.0457,  0.0034],\n",
      "        [-0.0845,  1.1376,  0.3131],\n",
      "        [ 0.5250, -0.8093,  0.4620],\n",
      "        [ 0.3376,  2.1880, -0.6366],\n",
      "        [ 1.0187,  0.5949, -0.8949]]) \n",
      "\n",
      "W (diagonal entries):\n",
      " tensor([2., 0., 3., 5., 0.]) \n",
      "\n",
      "Q^T W Q =\n",
      " tensor([[ 1.0000e+00,  0.0000e+00,  5.9605e-08],\n",
      "        [ 0.0000e+00,  1.0000e+00, -2.9802e-08],\n",
      "        [ 5.9605e-08,  0.0000e+00,  1.0000e+00]]) \n",
      "\n",
      "Energy (weighted)   =  3.2819647789001465\n",
      "Energy (unweighted) =  3.2819645404815674\n",
      "Difference          =  2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Setup random A and diagonal weights (some zero)\n",
    "m, n = 5, 3\n",
    "A = torch.randn(m, n)\n",
    "W_vals = torch.tensor([2.0, 0.0, 3.0, 5.0, 0.0])  # shape (5,)\n",
    "\n",
    "print(\"A:\\n\", A, \"\\n\")\n",
    "print(\"W (diagonal entries):\\n\", W_vals, \"\\n\")\n",
    "\n",
    "# 2) Identify nonzero rows\n",
    "mask = (W_vals > 0)\n",
    "A_sub = A[mask, :]          # submatrix with nonzero-weight rows\n",
    "w_sub = W_vals[mask]        # corresponding nonzero weights\n",
    "\n",
    "# 3) Form B = sqrt(W_sub)*A_sub, then do unweighted QR\n",
    "W_sub_sqrt = w_sub.sqrt()              # shape (m_sub,)\n",
    "B = W_sub_sqrt.unsqueeze(-1) * A_sub   # multiply each row by sqrt(w_sub)\n",
    "Q_std, R = torch.linalg.qr(B, mode='reduced')   # unweighted QR\n",
    "\n",
    "# 4) Convert Q_std -> Q_sub = invsqrt(W_sub)*Q_std\n",
    "W_sub_invsqrt = 1.0 / W_sub_sqrt\n",
    "Q_sub = W_sub_invsqrt.unsqueeze(-1) * Q_std\n",
    "\n",
    "# 5) Reassemble a full Q of shape (m, n), zero in rows where W=0\n",
    "Q = torch.zeros_like(A)  # same shape (5,3)\n",
    "Q[mask, :] = Q_sub\n",
    "\n",
    "# Check Q^T W Q = I\n",
    "W_mat = torch.diag(W_vals)\n",
    "lhs = Q.T @ W_mat @ Q\n",
    "print(\"Q^T W Q =\\n\", lhs, \"\\n\")\n",
    "\n",
    "# 6) Demonstrate that (Q R alpha)^T W (Q R alpha) = ||R alpha||^2\n",
    "alpha = torch.randn(n, 1)\n",
    "u_weighted = Q @ (R @ alpha)             # (m x 1)\n",
    "energy_weighted = (u_weighted.T @ W_mat @ u_weighted).item()\n",
    "energy_unweighted = (R @ alpha).norm().pow(2).item()\n",
    "\n",
    "print(\"Energy (weighted)   = \", energy_weighted)\n",
    "print(\"Energy (unweighted) = \", energy_unweighted)\n",
    "print(\"Difference          = \", abs(energy_weighted - energy_unweighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2])\n",
      "R_alpha shape is torch.Size([2, 2, 1])\n",
      "torch.Size([2, 4, 2])\n",
      "torch.Size([2, 2, 1])\n",
      "Q^T W Q =\n",
      "tensor([[[1., -0.],\n",
      "         [-0., 1.]],\n",
      "\n",
      "        [[1., 0.],\n",
      "         [0., 1.]]])\n",
      "torch.Size([2, 2, 1])\n",
      "Energy (weighted) per batch =  tensor([[2.0000],\n",
      "        [2.0000]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# Create batched inputs\n",
    "B_re = torch.rand((batch_size, 4, 2))\n",
    "B_im = torch.rand((batch_size, 4, 2))\n",
    "B = B_re + 1j * B_im\n",
    "\n",
    "un_alpha = torch.rand((batch_size, 2))\n",
    "un_alpha = torch.complex(un_alpha, torch.zeros_like(un_alpha))\n",
    "\n",
    "W_vals = torch.tensor([2.0, 0.0, 3.0, 5.0])  # (batch_size, 4)\n",
    "\n",
    "mask = (W_vals > 0)  # (batch_size, 4)\n",
    "\n",
    "# Handle each batch independently since QR is not batched\n",
    "Q = torch.zeros_like(B)  # (batch_size, 4, 3)\n",
    "\n",
    "B_sub = B[:,mask,:]          # (n_nonzero, 3)\n",
    "\n",
    "w_sub = W_vals[mask]        # (n_nonzero,)\n",
    "\n",
    "W_sub_sqrt = w_sub.sqrt()\n",
    "W_sub_invsqrt = 1.0 / W_sub_sqrt\n",
    "\n",
    "W_sub_sqrt_mat = torch.diag(W_sub_sqrt).unsqueeze(0).expand(B.shape[0],-1,-1)  # (n_nonzero, n_nonzero)\n",
    "W_sub_invsqrt = torch.diag(W_sub_invsqrt).unsqueeze(0).expand(B.shape[0],-1,-1)  # (n_nonzero, n_nonzero)\n",
    "\n",
    "W_sub_sqrt_mat = torch.complex(W_sub_sqrt_mat, torch.zeros_like(W_sub_sqrt_mat))\n",
    "W_sub_invsqrt = torch.complex(W_sub_invsqrt, torch.zeros_like(W_sub_invsqrt))\n",
    "\n",
    "B_sub = W_sub_sqrt_mat @ B_sub\n",
    "\n",
    "Q_std, R = torch.linalg.qr(B_sub, mode='reduced')\n",
    "\n",
    "Q_sub = W_sub_invsqrt @ Q_std\n",
    "\n",
    "\n",
    "Q[:,mask,:]   = Q_sub\n",
    "\n",
    "print(R.shape)\n",
    "print(un_alpha.shape)\n",
    "R_alpha = (R @ un_alpha.unsqueeze(-1))\n",
    "print(f'R_alpha shape is {R_alpha.shape}')\n",
    "energy_unweighted = torch.norm(R_alpha, p=2, dim=1, keepdim=True)  # keeping batch dimension\n",
    "R_alpha = R_alpha / energy_unweighted * torch.sqrt(torch.tensor(2.0))\n",
    "\n",
    "# Create batched diagonal weight matrix\n",
    "W_mat = torch.complex(torch.diag_embed(W_vals), torch.zeros_like(torch.diag_embed(W_vals)))\n",
    "\n",
    "print(Q.shape)\n",
    "print(R_alpha.shape)\n",
    "u_weighted = (Q @ R_alpha)  # (batch_size, 4, 1)\n",
    "energy_weighted = torch.diagonal(\n",
    "    torch.bmm(\n",
    "        torch.bmm(u_weighted.transpose(-2,-1).conj(), W_mat.unsqueeze(0).expand(batch_size, -1, -1)),\n",
    "        u_weighted\n",
    "    ),\n",
    "    dim1=-2, dim2=-1\n",
    ").real\n",
    "\n",
    "#print the product Q^TWQ\n",
    "QTWQ = torch.bmm(torch.bmm(Q.transpose(-2,-1).conj(), W_mat.unsqueeze(0).expand(batch_size, -1, -1)), Q)\n",
    "print(\"Q^T W Q =\")\n",
    "#print rounded\n",
    "print(QTWQ.real.round())\n",
    "\n",
    "\n",
    "\n",
    "print(R_alpha.shape)\n",
    "\n",
    "print(\"Energy (weighted) per batch = \", energy_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n",
      "tensor([ True,  True, False,  True,  True])\n",
      "tensor([1., 2., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "tensy = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "\n",
    "print(tensy)\n",
    "mask = (torch.ones_like(tensy, dtype=torch.bool)).bool()\n",
    "mask[2] = False\n",
    "\n",
    "print(mask)\n",
    "\n",
    "print(tensy[mask])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
